---
title: G√©rer un robots.txt simple pour le r√©f√©rencement gr√¢ce √† Nginx Proxy Manager
description: Guide simple et stable pour impl√©menter un fichier robots.txt statique pour Wiki.js gr√¢ce √† Nginx Proxy Manager (NPM). Autorise le r√©f√©rencement public et s√©curise le tableau de bord d'administration.
published: true
date: 2025-11-29T18:04:26.219Z
tags: nginx, proxy, npm, s√©curit√©, seo, robots.txt, wiki.js
editor: markdown
dateCreated: 2025-11-29T18:04:26.219Z
---

Ce guide utilise la m√©thode du **fichier statique** (`alias`) pour impl√©menter un `robots.txt` simple et stable pour votre site derri√®re **Nginx Proxy Manager (NPM)**. Nous prenons comme **exemple l'application Wiki.js** et le domaine **`wiki.blablalinux.be`** pour illustrer la protection des chemins d'administration sensibles.

-----

## 1\. Principe de la m√©thode du fichier statique

Le fait de demander √† Nginx de servir un **fichier statique** existant sur le disque via l'instruction `alias` est la solution la plus stable pour contourner les probl√®mes de limite de longueur de cha√Æne et garantir un fonctionnement sans erreur dans **NPM**.

### Pr√©-requis

1.  Avoir acc√®s au syst√®me de fichiers du serveur ou au volume Docker o√π **NPM** stocke ses donn√©es.

-----

## 2\. Contenu du fichier robots.txt simple (Exemple Wiki.js)

Vous devez cr√©er ou mettre √† jour le fichier sur votre syst√®me de fichiers.

  * **Chemin sugg√©r√© (√† adapter) :** `/data/seo/votre-application/robots.txt`

Ce contenu est la **configuration de base id√©ale** : elle autorise tous les robots √† indexer le contenu public, tout en prot√©geant un chemin d'administration sensible (le tableau de bord Wiki.js dans cet exemple).

```text
Sitemap: https://votre-domaine.com/sitemap.xml

# 1. R√àGLES POUR LES ROBOTS STANDARDS (AUTORIS√âS & S√âCURIS√âS)
# Cette r√®gle autorise le r√©f√©rencement de tout le contenu public.
User-agent: *
Disallow: /a/dashboard # Bloque l'acc√®s au panneau d'administration de Wiki.js (√Ä ADAPTER)
Allow: /
```

> **üëâ N'oubliez pas d'adapter la ligne `Sitemap:` avec votre propre nom de domaine et le chemin `Disallow:` avec les chemins sensibles de votre application.**

> **Exemple sp√©cialis√© :** Pour voir un exemple de `robots.txt` qui **bloque √©galement les robots d'intelligence artificielle**, vous pouvez consulter celui de Wiki.js Blabla Linux : [https://wiki.blablalinux.be/robots.txt](https://wiki.blablalinux.be/robots.txt).

-----

## 3\. Configuration dans Nginx Proxy Manager

Collez ce bloc Nginx dans l'onglet **Advanced** de l'h√¥te proxy de votre site. **Il est crucial d'adapter la directive `alias` pour qu'elle corresponde exactement au chemin o√π vous avez cr√©√© votre fichier statique.**

```nginx
# Bloc pour servir le fichier robots.txt statique (Solution stable via NPM)
location = /robots.txt {
    # Alias : ADAPTEZ CE CHEMIN avec le chemin r√©el de votre fichier sur le serveur
    alias /data/seo/votre-application/robots.txt; 
    
    # Assurer le bon Content-Type pour le SEO
    add_header Content-Type text/plain;
    charset utf-8;
    
    # Correction des probl√®mes potentiels de compression/artefacts
    gzip off;
    proxy_set_header Accept-Encoding ""; 
}

> **‚ö†Ô∏è REMARQUE IMPORTANTE :** Dans la ligne `alias`, vous devez imp√©rativement remplacer `/votre-application` par le nom de dossier que vous avez choisi pour stocker votre fichier `robots.txt`. Par exemple : `/data/seo/wikijs/robots.txt`.
```

-----

## 4\. V√©rification finale

Apr√®s avoir cr√©√© le fichier statique et sauvegard√© la configuration dans NPM :

1.  Acc√©dez √† l'URL : `https://votre-domaine.com/robots.txt`.
2.  Le contenu doit s'afficher parfaitement, confirmant que votre site est pr√™t pour l'indexation, avec les chemins sensibles prot√©g√©s.