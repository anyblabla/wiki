---
title: Bloquer les robots d‚ÄôIA directement via NGINX Proxy Manager
description: Apprenez √† centraliser le blocage des principaux crawlers d‚ÄôIA dans NGINX Proxy Manager (NPM) en utilisant un fichier de configuration custom. Une alternative efficace aux modifications des fichiers robots.txt individuels.
published: true
date: 2025-12-15T23:32:58.582Z
tags: nginx, proxy, npm, blocage-crawlers, robots, ia, ai, crawlers
editor: markdown
dateCreated: 2025-12-15T23:32:58.582Z
---

## Introduction
Plut√¥t que de modifier les fichiers `robots.txt` de chaque site individuellement (ce qui demande de maintenir des configurations s√©par√©es) :

* Exemple Gitea : [https://wiki.blablalinux.be/fr/robots-txt-gitea-blocage-ia](https://wiki.blablalinux.be/fr/robots-txt-gitea-blocage-ia)
* Exemple WordPress : [https://wiki.blablalinux.be/fr/robots-txt-wordpress-anti-ia](https://wiki.blablalinux.be/fr/robots-txt-wordpress-anti-ia)

... nous allons agir de mani√®re beaucoup plus **efficace et centralis√©e** !

Nous allons impl√©menter le blocage directement au niveau du **proxy inverse**, ici via **NGINX Proxy Manager (NPM)**. Cette m√©thode permet de rejeter la connexion pour les User-Agents cibl√©s *avant* qu'ils n'atteignent l'application cible, √©conomisant ainsi des ressources et simplifiant l'administration.

## 1. Comprendre l‚Äôarchitecture des configurations personnalis√©es de NPM
NGINX Proxy Manager offre des **points d'insertion (`include**`) bien d√©finis dans la configuration NGINX g√©n√©r√©e. Ces points vous permettent d'ajouter des directives personnalis√©es sans modifier les fichiers de configuration de base.

Voici la liste des points d'insertion disponibles dans le r√©pertoire `/data/nginx/custom/` (selon la documentation NPM) :

| Fichier de Configuration Personnalis√© | Emplacement d'Inclusion | Port√©e |
| --- | --- | --- |
| `/data/nginx/custom/root_top.conf` | En haut de `nginx.conf` | Globale |
| `/data/nginx/custom/http_top.conf` | En haut du bloc `http` principal | Globale (HTTP/HTTPS) |
| **`/data/nginx/custom/server_proxy.conf`** | **Fin de chaque bloc `server` de proxy** | **Par h√¥te (recommand√©)** |
| `/data/nginx/custom/http.conf` | Fin du bloc `http` principal | Globale (HTTP/HTTPS) |
| `/data/nginx/custom/root.conf` | √Ä la toute fin de `nginx.conf` | Globale |
| *Autres...* | *Voir la documentation pour `events`, `stream`, etc.* | *Sp√©cifique* |

> **‚ö†Ô∏è Attention : cr√©ation des fichiers**
> **Ces fichiers de configuration personnalis√©s n'existent pas par d√©faut !** Ils sont facultatifs. Vous devez imp√©rativement cr√©er le fichier souhait√© (dans notre cas, `server_proxy.conf`) dans le r√©pertoire `/data/nginx/custom/` avant d'y placer votre code.

Pour plus de d√©tails sur les points d'insertion, consultez la documentation officielle :
‚û°Ô∏è **[NGINX Proxy Manager - Documentation Configuration Avanc√©e](https://nginxproxymanager.com/advanced-config/)**

## 2. Impl√©mentation du blocage anti-IA
Nous allons utiliser le fichier `/data/nginx/custom/server_proxy.conf` pour cibler uniquement les configurations de proxy (les plus courantes pour les services expos√©s).

### √âtape A : cr√©er et √©diter le fichier
Via votre console Linux sur votre h√¥te (Proxmox/Docker) :

```bash
# Assurez-vous d'√™tre dans le bon volume de donn√©es de NPM
# Exemple de cr√©ation (si le volume est mont√© quelque part)
mkdir -p /chemin/vers/data/nginx/custom/
touch /chemin/vers/data/nginx/custom/server_proxy.conf
nano /chemin/vers/data/nginx/custom/server_proxy.conf

```

### √âtape B : ins√©rer le code de blocage
Ajoutez le bloc de code suivant √† l'int√©rieur de `server_proxy.conf` :

```nginx
# --- BLOCAGE CENTRALIS√â DES ROBOTS D'IA ---

# 1. Initialisation de la variable de blocage
set $block 0;

# 2. V√©rification des User-Agents des IA/Crawlers
# Cette expression r√©guli√®re v√©rifie l'en-t√™te http_user_agent
if ($http_user_agent ~* "(AddSearchBot|AI2Bot|AI2Bot\-DeepResearchEval|Ai2Bot\-Dolma|aiHitBot|amazon\-kendra|Amazonbot|AmazonBuyForMe|Andibot|Anomura|anthropic\-ai|Applebot|Applebot\-Extended|atlassian\-bot|Awario|bedrockbot|bigsur\.ai|Bravebot|Brightbot\ 1\.0|BuddyBot|Bytespider|CCBot|Channel3Bot|ChatGLM\-Spider|ChatGPT\ Agent|ChatGPT\-User|Claude\-SearchBot|Claude\-User|Claude\-Web|ClaudeBot|Cloudflare\-AutoRAG|CloudVertexBot|cohere\-ai|cohere\-training\-data\-crawler|Cotoyogi|Crawl4AI|Crawlspace|Datenbank\ Crawler|DeepSeekBot|Devin|Diffbot|DuckAssistBot|Echobot\ Bot|EchoboxBot|FacebookBot|facebookexternalhit|Factset_spyderbot|FirecrawlAgent|FriendlyCrawler|Gemini\-Deep\-Research|Google\-CloudVertexBot|Google\-Extended|Google\-Firebase|Google\-NotebookLM|GoogleAgent\-Mariner|GoogleOther|GoogleOther\-Image|GoogleOther\-Video|GPTBot|iAskBot|iaskspider|iaskspider/2\.0|IbouBot|ICC\-Crawler|ImagesiftBot|imageSpider|img2dataset|ISSCyberRiskCrawler|Kangaroo\ Bot|KlaviyoAIBot|KunatoCrawler|laion\-huggingface\-processor|LAIONDownloader|LCC|LinerBot|Linguee\ Bot|LinkupBot|Manus\-User|meta\-externalagent|Meta\-ExternalAgent|meta\-externalfetcher|Meta\-ExternalFetcher|meta\-webindexer|MistralAI\-User|MistralAI\-User/1\.0|MyCentralAIScraperBot|netEstate\ Imprint\ Crawler|NotebookLM|NovaAct|OAI\-SearchBot|omgili|omgilibot|OpenAI|Operator|PanguBot|Panscient|panscient\.com|Perplexity\-User|PerplexityBot|PetalBot|PhindBot|Poggio\-Citations|Poseidon\ Research\ Crawler|QualifiedBot|QuillBot|quillbot\.com|SBIntuitionsBot|Scrapy|SemrushBot\-OCOB|SemrushBot\-SWA|ShapBot|Sidetrade\ indexer\ bot|Spider|TerraCotta|Thinkbot|TikTokSpider|Timpibot|TwinAgent|VelenPublicWebCrawler|WARDBot|Webzio\-Extended|webzio\-extended|wpbot|WRTNBot|YaK|YandexAdditional|YandexAdditionalBot|YouBot|ZanistaBot)") {
    set $block 1;
}

# 3. EXCEPTION : Autoriser l'acc√®s √† robots.txt
if ($request_uri = "/robots.txt") {
    set $block 0;
}

# 4. Ex√©cution du blocage (retour 403 Forbidden)
if ($block) {
    return 403;
}
# --- FIN BLOCAGE ANTI-IA ---

```

> **üìå Note sur la maintenance**
> Les User-Agents des robots d'IA sont en constante √©volution et de nouveaux crawlers apparaissent r√©guli√®rement. Il est crucial de maintenir la liste ci-dessus √† jour. La liste compl√®te des bots couramment bloqu√©s est inspir√©e de configurations communautaires telles que :
> ‚û°Ô∏è **[AI Robots NGINX Block List sur GitHub](https://github.com/ai-robots-txt/ai.robots.txt/blob/main/nginx-block-ai-bots.conf)**

## 3. Application de la configuration
Apr√®s avoir enregistr√© votre fichier `server_proxy.conf`, vous devez **red√©marrer le conteneur NGINX Proxy Manager** pour qu'il inclue ce nouveau fichier dans la configuration principale et recharge le service NGINX :

```bash
docker restart <nom_du_conteneur_npm>

```

*(Remplacez `<nom_du_conteneur_npm>` par le nom de votre conteneur NPM, par exemple `nginx-proxy-manager`).*

## 4. Test de v√©rification du blocage (depuis votre terminal Linux)
Utilisez l'outil **`curl`** avec l'option `-A` (pour sp√©cifier le User-Agent) afin de simuler l'acc√®s d'un robot bloqu√©, puis d'un utilisateur normal.

### A. Tester avec un robot d'IA (attendu : code 403)
Ex√©cutez la commande suivante en rempla√ßant `votre-site.com` par l'une de vos URL g√©r√©es par NPM :

```bash
# Simuler GPTBot tentant d'acc√©der √† votre page d'accueil
curl -A "GPTBot" -I https://votre-site.com/

```

**R√©sultat attendu :** Vous devriez voir un code de r√©ponse HTTP **`403 Forbidden`**.

### B. Tester l'exclusion `robots.txt` (attendu : code 200/301/302)
Ciblez `/robots.txt` avec le m√™me User-Agent bloqu√© :

```bash
# Simuler GPTBot tentant d'acc√©der √† robots.txt
curl -A "GPTBot" -I https://votre-site.com/robots.txt

```

**R√©sultat attendu :** Vous devriez obtenir une r√©ponse de succ√®s (`200 OK`) ou une redirection, mais **pas de `403**`.

### C. Tester avec un utilisateur standard (attendu : code 200 ou 301/302)
V√©rifiez l'acc√®s normal sans sp√©cifier de User-Agent bloqu√© :

```bash
# Simuler un navigateur standard
curl -I https://votre-site.com/

```

**R√©sultat attendu :** Vous devriez recevoir une r√©ponse de succ√®s ou la redirection normale de votre application.

Si ces trois tests renvoient les codes attendus, votre configuration est fonctionnelle !